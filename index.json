
[{"content":"","date":"August 22, 2025","externalUrl":null,"permalink":"/tags/ai/","section":"Tags","summary":"","title":"AI","type":"tags"},{"content":"","date":"August 22, 2025","externalUrl":null,"permalink":"/","section":"Kevin Chiu","summary":"","title":"Kevin Chiu","type":"page"},{"content":"","date":"August 22, 2025","externalUrl":null,"permalink":"/tags/learning/","section":"Tags","summary":"","title":"Learning","type":"tags"},{"content":"","date":"August 22, 2025","externalUrl":null,"permalink":"/tags/llm/","section":"Tags","summary":"","title":"LLM","type":"tags"},{"content":"","date":"August 22, 2025","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":" Sometimes LLMs don\u0026rsquo;t have the right answer. Why?\nI noticed something strange with LLMs. Multiple models from different providers respond with the same wrong answer to a simple multiple choice question.\nHere\u0026rsquo;s a table of the models and their answers. I\u0026rsquo;m keeping the question secret for now so I can use it to test future models, but here\u0026rsquo;s a placeholder:\nThis is a question about the number of intersecting points between seven parallel lines on a 2-dimensional x-y plane. Assume each line is placed such that it intersects the x-axis one unit away from the nearest line. If the lines are rotated 180 degrees, at how many points do they intersect?\na) 7 b) 3 c) 0 d) 5 e) 2\nThe correct answer is c) 0, but so far, every LLM has answered d) 5.\nDate Tested Provider Model Answer 12/21/2024 OpenAI ChatGPT 4o d 12/21/2024 xAI Grok 2 d 12/21/2024 Meta Llama 3.1 70B d 12/21/2024 Quora Poe d 12/21/2024 Google Gemini 2.0 Experimental Advanced d 12/25/2024 Meta Llama 3.3 70B d 12/25/2024 Anthropic Claude Sonnet 3.5 d 01/16/2024 xAI Grok Beta d 01/16/2024 DeepSeek DeepSeek R1 Distill Llama 70B GGUF d 01/23/2024 DeepSeek DeepSeek Chat d 08/16/2024 OpenAI OpenAI GPT OSS 20B d TODO: What\u0026rsquo;s going on with reasoning? Although every model came up with the same answer, the reasoning ones had different reasoning traces. Maybe reasoning is just annotating the residual stream and not actually affecting the output? Is reasoning real?\nTODO: Replicate on the smallest model that can be hooked up to interpretability tools and figure out why the wrong answer is chosen. See if the pattern is consistent across different models.\nIs everyone training on the same data? # The most basic explanation for this phenomenon is that these models were trained on similar datasets. As I recall from distributed systems class in undergrad, Google\u0026rsquo;s crawl data was only around 8 terabytes uncompressed. I would guess that the web has grown significantly since then. Common Crawl has a text portion of about 8 terabytes compressed. Running the text index through Claude to estimate its size gives me about 20TB uncompressed, surprisingly small compared to what I thought it would be. Maybe there\u0026rsquo;s about 10x more useful text data out there\u0026hellip; and even then it would easily fit on a single machine. It\u0026rsquo;s reasonable to think that everyone could be training on the same data since it\u0026rsquo;s not that hard to gather most of it.\nThe common training data for most LLMs likely consists of:\nOpen web / crawled data (Common Crawl, FineWeb, Wikipedia, etc.) More \u0026ldquo;information-rich\u0026rdquo; caches of text like books, academic papers, paywalled sites In some cases, output from other LLMs (Sometimes DeepSeek says it\u0026rsquo;s ChatGPT) Maybe pirated materials, such as Anna\u0026rsquo;s Archive, The Pile, or Books2. There really doesn\u0026rsquo;t seem to be that much good text data out there. So, as we train more and more models towards the same distribution target, we\u0026rsquo;re likely to see more convergence. In the limit, the only thing differentiating models will be things other than being able to generate an \u0026ldquo;in distribution\u0026rdquo; answer, it will be more about alignment and other model features, such as efficiency and interpretability.\nNote: I suppose if some labs strike exclusive data deals, such as Reddit\u0026rsquo;s deals with Google and OpenAI, that could also be a differentiator.\nLooking Forward # As we move toward increasingly powerful models, will we see greater diversity in their outputs, or will they continue to experience convergent evolution, clustering outputs around the same learned distribution due to a lack of novel data? If model architectures are just increasingly better at representing the data, then the real important thing is the data. Also, if everyone eventually converges to the same data and resulting model abilities, then alignment is all that matters.\n","date":"August 22, 2025","externalUrl":null,"permalink":"/posts/1755636469288-something-strange-with-llms/","section":"Posts","summary":"LLMs from different companies produce identical wrong answers. Is it the data?","title":"Something Strange with LLMs","type":"posts"},{"content":"","date":"August 22, 2025","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":" In \u0026ldquo;Ex Machina,\u0026rdquo; an AI distilled from search engine data is embodied in a humanoid form.\n\u0026ldquo;Any sufficiently advanced technology is indistinguishable from magic.\u0026rdquo;\nArthur C. Clarke Large Language Models are magic. They\u0026rsquo;ve become the artificial intelligence foretold by science fiction, yet no satisfying explanation exists for how they work. It\u0026rsquo;s as if we threw text into a pressure cooker and intelligent life emerged.\nAs a kid, I was nerd-sniped by magic tricks. I spent countless hours figuring out how they worked. That natural curiosity followed me throughout life. Computers, the Internet—these seemed magical at first, but they were well-documented and could be understood with enough effort. LLMs, however, remain stubbornly mysterious despite rapid advancements that suggest someone, somewhere, knows how they work \u0026ndash; how lossy compression and decompression of data seems to have created intelligence.\nYet hunting around reveals even the insiders at frontier labs are still working to understand the inner workings of LLMs. Models continue to march ahead in capabilities, but the underlying mechanisms remain elusive. After speaking with people who work directly on LLMs and also doing a bit of background reading, I\u0026rsquo;ve come to realize something that insiders already know: we possess, at best, piecewise knowledge that lifts the edge of veil just a little bit at a time.\nIn 2019, OpenAI\u0026rsquo;s GPT-2 made waves by generating text that could masquerade as human-written. People worried it would flood the internet with AI-generated articles designed to capture attention and ad dollars. I initially dismissed it as merely a mega-sized hidden Markov model—a \u0026ldquo;stochastic parrot\u0026rdquo; repeating patterns without understanding.\nThen came ChatGPT.\nReleased quietly in late 2022, it exploded into public consciousness. Suddenly, millions were chatting with a GPT model post-trained with RLHF (Reinforcement Learning from Human Feedback). Many couldn\u0026rsquo;t distinguish between the chatbot and a human. The Turing Test, an ambitious goal that was perpetually on the horizon, was passed in the blink of an eye.\nWe\u0026rsquo;re living a timeline sci-fi movies warned us about. The 2014 film \u0026ldquo;Ex Machina\u0026rdquo; depicted a near future where an AI is created by distilling online data into artificial brains installed in humanoid robots in an attempt to pass a physical Turing Test. With humanoid robots and GenAI startups springing up everywhere, fiction has become reality with startling speed. The film also presciently highlighted something we\u0026rsquo;re grappling with now: the critical importance of understanding how AIs actually work, and the implications for AI safety. (Spoiler: It\u0026rsquo;s important.)\nIn my next post I\u0026rsquo;ll highlight an anomaly I observed while poking around LLMs and how that led me to start pulling on the thread of how LLMs work.\n","date":"August 17, 2025","externalUrl":null,"permalink":"/posts/1755462557996-indistinguishable-from-magic/","section":"Posts","summary":"Large Language Models are magic. It\u0026rsquo;s as if we threw text into a pressure cooker and intelligent life emerged.","title":"Indistinguishable from Magic","type":"posts"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]